{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:3px solid black; padding:20px\">\n",
    "    \n",
    "\n",
    "## * 아파트 실거래가 예측 경진대회\n",
    "\n",
    "### + 배경\n",
    "+ 국토부 실거래가 데이터를 가공한 유사 데이터로 진행되었으며, 아파트 실거래가를 예측하는 문제  \n",
    "\n",
    "\n",
    "+ 현재는 교육용 대회로 전환됨\n",
    "+ 대회 링크: https://dacon.io/competitions/open/235537/overview/\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVckQF0EvGP8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특성별 데이터 개수\n",
    "+ dong : 473\n",
    "+ addr_kr : 12533\n",
    "+ jibun : 8961\n",
    "+ apartment_id : 12533\n",
    "+ apt : 10440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Data load >\n",
    "\n",
    "+ transaction_real_price는 min, max의 차이, std가 크므로 로그를 취해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrYQAogzvIMM"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "park= pd.read_csv('./data/park.csv')\n",
    "day_care_center = pd.read_csv('./data/day_care_center.csv')\n",
    "# park.gu 에서 추출하여 labeling 후 csv로 저장한 파일\n",
    "gu = pd.read_csv('./data/gu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.transaction_real_price.describe())\n",
    "\n",
    "train.transaction_real_price = np.log1p(train.transaction_real_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < preprocessing and feature engineering >\n",
    "### Center, Park.csv를 통한 특성 추출\n",
    "\n",
    "+ 유치원 타입별로 차이가 있는줄 알았지만, 그렇지 않음\n",
    "+ 결과에 영향을 거의 주지 않음\n",
    "+ 공원은 '동'단위로 나누어져 있으며, 대체로 공원이 있는 동의 집값이 더 높음\n",
    "+ 묘지공원을 포함한 동은 다른 동에 비해 집값이 낮음\n",
    "  \n",
    "  \n",
    "+ 공원이 없는 동은 전부 0으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_day_care_baby_num = day_care_center.groupby(['gu'])['day_care_baby_num'].agg({'mean'})\n",
    "\n",
    "park = pd.merge(park, gu_day_care_baby_num, how = 'left', on = 'gu')\n",
    "park.fillna(0, inplace = True)\n",
    "park = park.rename(columns = {'mean':'day_care_baby_num_mean'})\n",
    "ppark = park.loc[:, ['dong', 'park_type', 'day_care_baby_num_mean']] \n",
    "ppark = ppark[['dong']].join(pd.get_dummies(ppark['park_type'])).join(ppark['day_care_baby_num_mean']).groupby('dong').max()\n",
    "\n",
    "train = pd.merge(train, ppark, how = 'left', on = 'dong')\n",
    "test = pd.merge(test, ppark, how = 'left', on = 'dong')\n",
    "\n",
    "train.fillna(0, inplace = True)\n",
    "test.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 전용면적을 평 단위로 나누고, 평당 가격 특성을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('floor','transaction_real_price',data=train.sample(1000),order=2,scatter_kws={'alpha':0.1})\n",
    "\n",
    "train['hangang']=train['dong'].isin(['성수동1가','삼성동','이촌동','공덕동','서교동','한강로3가','목동']).astype(int)\n",
    "test['hangang']=test['dong'].isin(['성수동1가','삼성동','이촌동','공덕동','서교동','한강로3가','목동']).astype(int)\n",
    "\n",
    "train['living'] = train['exclusive_use_area'] / 3.30578532\n",
    "test['living'] = test['exclusive_use_area'] / 3.30578532\n",
    "\n",
    "train['per_price'] = train['transaction_real_price'] / train['living']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 동단위로 아파트의 총 개수를 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train,train.groupby(['dong']).size().reset_index(name='dong_count'),on=['dong'],how='left')\n",
    "test=pd.merge(test,train.groupby(['dong']).size().reset_index(name='dong_count'),on=['dong'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 동 내부에서도 지번별로 아파트 실거래가가 다를거라고 예상함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dong_jibun_price = train.groupby(['dong', 'jibun'])['transaction_real_price'].agg({'mean'}).reset_index()\n",
    "\n",
    "train = pd.merge(train, dong_jibun_price, how = 'left', on = ['dong', 'jibun'])\n",
    "train = train.rename(columns = {'mean':'dong_jibun_price'})\n",
    "\n",
    "test = pd.merge(test, dong_jibun_price, how = 'left', on = ['dong', 'jibun'])\n",
    "test = test.rename(columns = {'mean':'dong_jibun_price'})\n",
    "test.fillna(np.mean(test['dong_jibun_price']), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 동별로 실거래가 평균과 분산 특성 추가\n",
    "+ 도시 및 지번별 실거래가 평균값 특성 추가\n",
    "+ 도시 및 아파트별 실거래가 평균값 특성 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uuyffNjVJvr0"
   },
   "outputs": [],
   "source": [
    "# dong encoding\n",
    "dong_price = train.groupby(['dong'])['per_price'].agg({'mean', 'var'}).reset_index()\n",
    "train = pd.merge(train, dong_price, how = 'left', on = 'dong')\n",
    "test = pd.merge(test, dong_price, how = 'left', on = 'dong')\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['dong_mean'] = df['mean'] * df['living']\n",
    "    df['dong_var'] = df['var'] * df['living']\n",
    "    del df['mean'], df['var']\n",
    "    \n",
    "train = train.fillna(np.mean(train['dong_var']))\n",
    "\n",
    "# addr_kr encoding\n",
    "c_addr = train.groupby(['city', 'addr_kr'])['transaction_real_price'].agg({'mean'})\n",
    "train = pd.merge(train, c_addr, how = 'left', on = ['city', 'addr_kr' ])\n",
    "train = train.rename(columns = {'mean':'c_addr'})\n",
    "test = pd.merge(test, c_addr, how = 'left', on = ['city', 'addr_kr'])\n",
    "test = test.rename(columns = {'mean':'c_addr'})\n",
    "test.fillna(np.mean(test['c_addr']), inplace = True)\n",
    "\n",
    "# apartment_id encoding\n",
    "c_a = train.groupby(['city', 'apartment_id'])['transaction_real_price'].agg({'mean'}).reset_index()\n",
    "train = pd.merge(train, c_a, how = 'left', on = ['city', 'apartment_id'])\n",
    "train = train.rename(columns = {'mean':'c_a'})\n",
    "test = pd.merge(test, c_a, how = 'left', on = ['city', 'apartment_id'])\n",
    "test = test.rename(columns = {'mean':'c_a'})\n",
    "test.fillna(np.mean(test['c_a']), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 년, 월 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qvnjzyr1vNGD"
   },
   "outputs": [],
   "source": [
    "train['year'] = train['transaction_year_month'].apply(lambda x : str(x)[:4])\n",
    "train['month'] = train['transaction_year_month'].apply(lambda x : str(x)[4:])\n",
    "\n",
    "test['year'] = test['transaction_year_month'].apply(lambda x : str(x)[:4])\n",
    "test['month'] = test['transaction_year_month'].apply(lambda x : str(x)[4:])\n",
    "\n",
    "train['year'] = train['year'].astype(np.int)\n",
    "test['year'] = test['year'].astype(np.int)\n",
    "\n",
    "train['month'] = train['month'].astype(np.int)\n",
    "test['month'] = test['month'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ transaction_date 특성을 LabelEncoder를 통해 labeling 해줌\n",
    "+ 거래시점과 완성년도의 차이를 계산한 특성을 추가\n",
    "+ 아파트가 완성된 시점이 오래될수록 실거래가가 낮을 것이라고 판단\n",
    "+ 아파트가 30년이 지난 경우, 재건축되었을 확률이 높으므로 이를 특성으로 추가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJLrQukqvP2t"
   },
   "outputs": [],
   "source": [
    "# transaction_date labeling\n",
    "date_label = preprocessing.LabelEncoder()\n",
    "date_label.fit(train.transaction_date)\n",
    "train['transaction_date'] = date_label.transform(train.transaction_date)\n",
    "test['transaction_date'] = date_label.transform(test.transaction_date)\n",
    "\n",
    "# 거래시점과 완성년도의 차이\n",
    "train['transaction_diff'] = train['year'].astype(np.int) - train['year_of_completion']\n",
    "test['transaction_diff'] = test['year'].astype(np.int) - test['year_of_completion']\n",
    "\n",
    "train['is_rebuild']=(train['transaction_diff']>=30).astype(int)\n",
    "test['is_rebuild']=(test['transaction_diff']>=30).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 각 도시에서 거래시점과 완성년도의 차이름 기준으로 평균 특성을 추가함\n",
    "+ 가장 최근에 지어진 아파트일수록 기존 데이터에 존재하지 않아 최솟값으로 대체함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 도시에서 완성년도와 거래시점과 완성년도의 차이를 기준으로\n",
    "c_y_td = train.groupby(['city', 'year_of_completion', 'transaction_diff'])['transaction_real_price'].agg({'mean'}).reset_index()\n",
    "\n",
    "train = pd.merge(train, c_y_td, how = 'left', on = ['city', 'year_of_completion', 'transaction_diff'])\n",
    "train = train.rename(columns = {'mean':'c_y_td'})\n",
    "\n",
    "test = pd.merge(test, c_y_td, how = 'left', on = ['city', 'year_of_completion', 'transaction_diff'])\n",
    "test = test.rename(columns = {'mean':'c_y_td'})\n",
    "\n",
    "# 주로 낮은 년도의 수가 채워지지 않으므로 평균이 아닌 min값으로 채워준다  \n",
    "test.fillna(np.min(test['c_y_td']), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 각 도시에서 거래시점과 날짜를 기준으로 평균값 특성을 추가함\n",
    "+ 가장 최근인 2017.12월이 없기때문에 최댓값으로 이를 대체함\n",
    "+ 최근이기 때문에 실거래가의 평균이 높을 것으로 판단하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 도시에서 거래시점+날짜를 기준으로\n",
    "c_tym_td = train.groupby(['city', 'transaction_year_month', 'transaction_date'])['transaction_real_price'].agg({'mean'}).reset_index()\n",
    "\n",
    "train = pd.merge(train, c_tym_td, how = 'left', on = ['city', 'transaction_year_month', 'transaction_date'])\n",
    "train = train.rename(columns = {'mean':'c_tym_td'})\n",
    "\n",
    "test = pd.merge(test, c_tym_td, how = 'left', on = ['city', 'transaction_year_month', 'transaction_date'])\n",
    "test = test.rename(columns = {'mean':'c_tym_td'})\n",
    "\n",
    "# 가장 최근인 2017 12월이 없으므로 평균이 아닌 최댓값으로 채워준다 \n",
    "test.fillna(np.max(test['c_tym_td']), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 서울과 부산을 따로 학습시키기 위해 이를 나누어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_label = {'서울특별시':0,'부산광역시':1}\n",
    "\n",
    "train['city'] = train['city'].map(city_label)\n",
    "test['city'] = test['city'].map(city_label)\n",
    "\n",
    "seoul_transaction_id = test[test['city'] == 0]['transaction_id']\n",
    "busan_transaction_id = test[test['city'] == 1]['transaction_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OguE2IatMuWe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 에측에 사용되지 않을 열들 삭제\n",
    "del train['transaction_id']; del test['transaction_id']\n",
    "del train['apartment_id']; del test['apartment_id']\n",
    "del train['dong']; del test['dong']\n",
    "del train['apt']; del test['apt']\n",
    "del train['addr_kr']; del test['addr_kr']\n",
    "del train['jibun']; del test['jibun']\n",
    "del train['living']; del test['living']\n",
    "del train['transaction_year_month']; del test['transaction_year_month']\n",
    "del train['transaction_diff']; del test['transaction_diff']\n",
    "del train['transaction_date']; del test['transaction_date']\n",
    "del train['per_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Training >\n",
    "\n",
    "+ xgb로도 실험해보았지만, lgb의 경우가 더 좋은 성능을 보여주었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul_train = train[train['city'] == 0].reset_index(drop = True)\n",
    "seoul_test = test[test['city'] == 0].reset_index(drop = True)\n",
    "\n",
    "busan_train = train[train['city'] == 1].reset_index(drop = True)\n",
    "busan_test = test[test['city'] == 1].reset_index(drop = True)\n",
    "\n",
    "del seoul_train['city']; del seoul_test['city']\n",
    "del busan_train['city']; del busan_test['city']\n",
    "\n",
    "folds = KFold(n_splits=5,shuffle=True,random_state=15)\n",
    "\n",
    "param = {'num_leaves': 100,\n",
    "         'min_data_in_leaf': 15, \n",
    "         'objective':'regression',\n",
    "         'max_depth': 6,\n",
    "         'learning_rate': 0.1,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "train_columns = train.drop(['city', 'transaction_real_price'], axis = 1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 부산 학습 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "busan_predictions_ops=np.zeros(len(busan_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(busan_train.values, busan_train['transaction_real_price'].values)):\n",
    "    trn_data = lgb.Dataset(busan_train[train_columns].iloc[trn_idx],label=busan_train['transaction_real_price'].iloc[trn_idx])#,weight=game_by_game.loc[(game_by_game['year']<year)&(game_by_game['AB']>0)]['AB'].iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(busan_train[train_columns].iloc[val_idx],label=busan_train['transaction_real_price'].iloc[val_idx])#,weight=game_by_game.loc[(game_by_game['year']<year)&(game_by_game['AB']>0)]['AB'].iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=1000,\n",
    "                    early_stopping_rounds = 200)\n",
    "\n",
    "\n",
    "    busan_predictions_ops += clf.predict(busan_test[train_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "busan_predictions_ops/=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busan_sub = pd.DataFrame({'transaction_id':busan_transaction_id, \n",
    "                          'transaction_real_price':busan_predictions_ops})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 서울 학습 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seoul_predictions_ops=np.zeros(len(seoul_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(seoul_train.values, seoul_train['transaction_real_price'].values)):\n",
    "    trn_data = lgb.Dataset(seoul_train[train_columns].iloc[trn_idx],label=seoul_train['transaction_real_price'].iloc[trn_idx])#,weight=game_by_game.loc[(game_by_game['year']<year)&(game_by_game['AB']>0)]['AB'].iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(seoul_train[train_columns].iloc[val_idx],label=seoul_train['transaction_real_price'].iloc[val_idx])#,weight=game_by_game.loc[(game_by_game['year']<year)&(game_by_game['AB']>0)]['AB'].iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=1000,\n",
    "                    early_stopping_rounds = 200)\n",
    "\n",
    "\n",
    "    seoul_predictions_ops += clf.predict(seoul_test[train_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "seoul_predictions_ops/=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul_sub = pd.DataFrame({'transaction_id':seoul_transaction_id, \n",
    "                          'transaction_real_price':seoul_predictions_ops})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 제출 형식 > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat((busan_sub, seoul_sub), axis = 0)\n",
    "\n",
    "id = pd.read_csv('./data/submission.csv')\n",
    "id = id.loc[:, 'transaction_id']\n",
    "\n",
    "sub = pd.merge(id, concat_df, how = 'left', on = 'transaction_id')\n",
    "sub['transaction_real_price'] = np.expm1(sub['transaction_real_price'])\n",
    "\n",
    "sub.to_csv('./sub/my_submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
