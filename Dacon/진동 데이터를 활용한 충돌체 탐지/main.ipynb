{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:3px solid black; padding:20px\">\n",
    "    \n",
    "    \n",
    "## * 진동데이터 활용 충돌체 탐지 AI 경진대회\n",
    "\n",
    "### + 배경\n",
    "+ 원자력발전소 냉각재계통 내부에 존재하는 충돌체는 기기의 손상을 유발할 수 있습니다.<br>원전 현장에서는 기기의 이상징후를 조기에 진단하여 사고를 방지하고자 합니다.<br>본 대회에서는 기기의 이상징후를 조기에 진단할 수 인공지능 기술 연구 활성화를 목적으로 합니다.\n",
    "\n",
    "### + 기간\n",
    "+ 2020.06.01 ~ 2020.07.10\n",
    "\n",
    "\n",
    "+ 대회 링크: https://dacon.io/competitions/official/235614/overview/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 필요 라이브러리 Import >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTq0oy9bLHwN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt; %matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten,MaxPooling2D,BatchNormalization,Lambda, AveragePooling2D\n",
    "from tensorflow.keras.layers import Input, Concatenate, GlobalAveragePooling2D, ZeroPadding2D, SeparableConv2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "np.random.seed(777)\n",
    "random.seed(777)\n",
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Data Load and Preprocessing >\n",
    "+ 각 진동체가 움직이는 범위의 평균과 표준편차를 특성으로 만들고,\n",
    "+ 전체 진동체의 평균과 표준편차를 특성으로 만들어, X_data에 concat합니다.\n",
    "+ 푸리에 변환을 활용한 특성 추가, 다양한 평균 및 표준편차를 활용한 특성을 추가해보았지만 여기서 사용한 특성으로 가장 좋은 성능을 얻을 수 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hZmRxeUBW3v"
   },
   "outputs": [],
   "source": [
    "X_data = pd.read_csv('./data/train_features.csv')\n",
    "X_data_test = pd.read_csv('./data/test_features.csv')\n",
    "\n",
    "for col in ['S1', 'S2', 'S3', 'S4']:\n",
    "    col_mean = X_data[col].mean()\n",
    "    col_std = X_data[col].std()\n",
    "\n",
    "    X_data[col] = (X_data[col] - col_mean) / (col_std + 1e-10)\n",
    "    X_data_test[col] = (X_data_test[col] - col_mean) / (col_std + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2273,
     "status": "ok",
     "timestamp": 1594137423765,
     "user": {
      "displayName": "조휘용",
      "photoUrl": "",
      "userId": "03176435450859375087"
     },
     "user_tz": -540
    },
    "id": "DN4j_-K2CE6e",
    "outputId": "64c69f62-b336-4e6d-ee5a-19c58a944289"
   },
   "outputs": [],
   "source": [
    "# 학습용 데이터 전처리를 수행합니다.\n",
    "X_data = X_data.to_numpy()\n",
    "X_data = X_data[:,1:]\n",
    "X_data = X_data.reshape((2800, 375, 5))\n",
    "\n",
    "trainv_mean = np.mean(X_data[:, :, 1:-1], axis = 2)[..., np.newaxis]\n",
    "trainv_std = np.std(X_data[:, :, 1:-1], axis = 2)[..., np.newaxis]\n",
    "\n",
    "X_data = np.concatenate((X_data, trainv_mean),axis = -1)\n",
    "X_data = np.concatenate((X_data, trainv_std),axis = -1)\n",
    "X_data = X_data.reshape((2800, 375, 7, 1))\n",
    "print(X_data.shape)\n",
    "    \n",
    "Y_data = np.loadtxt('./data/train_target.csv',skiprows=1,delimiter=',')\n",
    "Y_data = Y_data[:,1:]\n",
    "print(Y_data.shape)\n",
    "\n",
    "# 테스트용 데이터 전처리를 수행합니다.\n",
    "X_data_test = X_data_test.to_numpy()\n",
    "X_data_test = X_data_test[:,1:]\n",
    "X_data_test = X_data_test.reshape((700, 375, 5))\n",
    "\n",
    "testv_mean = np.mean(X_data_test[:, :, 1:-1], axis = 2)[..., np.newaxis]\n",
    "testv_std = np.std(X_data_test[:, :, 1:-1], axis = 2)[..., np.newaxis]\n",
    "\n",
    "X_data_test = np.concatenate((X_data_test, testv_mean),axis = -1)\n",
    "X_data_test = np.concatenate((X_data_test, testv_std),axis = -1)\n",
    "X_data_test = X_data_test.reshape((700, 375, 7, 1))\n",
    "print(X_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XuTZchXLz4f"
   },
   "outputs": [],
   "source": [
    "X_train = X_data; Y_train = Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Loss Define >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6E94S9TqL3pC"
   },
   "outputs": [],
   "source": [
    "weight1 = np.array([1,1,0,0]) # x, y\n",
    "weight2 = np.array([0,0,1,1]) # M, V\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    \n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
    "\n",
    "def my_loss_E2(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    \n",
    "    return K.mean(K.square(divResult)*weight2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Model Define >\n",
    "+ Mish 활성화 함수를 사용했습니다.\n",
    "+ CNN을 통해 각 진동체의 특징을 활용하도록 합니다. 하지만 데이터의 형태가 (Batch_size, row, col, channel)로 구성되어 있기 떄문에\n",
    "+ CNN 2D를 사용하는 대신 (3, 1), (5, 1)과 같은 필터 크기를 사용토록 합니다.\n",
    "+ 아래 모델에서는 [3, 5, 7, 9, 11, 13, 15] 크기가 사용되었습니다.\n",
    "+ x, y(충격 좌표)는 함께 학습하고, M(질량)과 V(충돌 속도)는 따로 분리하여 학습하도록 합니다. 이는 weight2 변수를 통해 Loss에서 통제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mish(x):\n",
    "    return x * K.tanh(K.softplus(x))\n",
    "\n",
    "get_custom_objects().update({'mish': Mish})\n",
    "\n",
    "def conv_block(inputs, nf, fs, pool_size = (2, 1), padding = 'valid', activation = 'elu', downsample = True):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Conv2D(filters = nf, kernel_size = fs, padding = padding)(x)\n",
    "    x = Activation(activation)(x)\n",
    "    \n",
    "    if downsample:\n",
    "        x = MaxPooling2D(pool_size = pool_size)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g97wzIHoL5gx"
   },
   "outputs": [],
   "source": [
    "def get_model(train_target):\n",
    "    '''\n",
    "        #train_target: 0이면 x,y, 1이면 M, 2이면 V\n",
    "    '''\n",
    "    \n",
    "    activation = 'mish'\n",
    "    padding = 'valid'\n",
    "    nf = 16\n",
    "    fs_3 = (3, 1); fs_5 = (5, 1)\n",
    "    fs_7 = (7, 1); fs_9 = (9, 1)\n",
    "    fs_11 = (11, 1); fs_13 = (13, 1)\n",
    "    fs_15 = (15, 1)\n",
    "    \n",
    "    inputs = Input(shape = (375, 7, 1))\n",
    "\n",
    "    init_layer = conv_block(inputs, nf, (3, 1), padding = padding, activation = activation)\n",
    "    init_layer = conv_block(init_layer, nf * 2, (3, 1), padding = padding, activation = activation)\n",
    "    \n",
    "    x_3 = conv_block(init_layer, nf*16, (1, 1), pool_size = (2, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x_3 = conv_block(x_3, nf*4, fs_3, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_3 = conv_block(x_3, nf*8, fs_3, pool_size = (2, 1), padding = padding, activation = activation) # (42, 5, 256)\n",
    "    x_3 = conv_block(x_3, nf*16, fs_5, pool_size = (2, 1), padding = padding, activation = activation, downsample = False) # (42, 5, 256)\n",
    "    \n",
    "    x_5 = conv_block(init_layer, nf*16, (1, 1), pool_size = (2, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x_5 = conv_block(x_5, nf*4, fs_5, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_5 = conv_block(x_5, nf*8, fs_5, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_5 = conv_block(x_5, nf*16, fs_5, pool_size = (2, 1), padding = padding, activation = activation, downsample = False) # (44, 5, 128)\n",
    "    x_5 = ZeroPadding2D(padding = ((1, 0), 0))(x_5)\n",
    "\n",
    "    x_7 = conv_block(init_layer, nf*16, (1, 1), pool_size = (2, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x_7 = conv_block(x_7, nf*4, fs_7, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_7 = conv_block(x_7, nf*8, fs_7, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_7 = conv_block(x_7, nf*16, fs_7, pool_size = (2, 1), padding = padding, activation = activation, downsample = False) # (44, 5, 128)\n",
    "    x_7 = ZeroPadding2D(padding = ((5, 0), 0))(x_7)\n",
    "\n",
    "    x_9 = conv_block(init_layer, nf*16, (1, 1), pool_size = (2, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x_9 = conv_block(x_9, nf*8, fs_9, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_9 = conv_block(x_9, nf*16, fs_9, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "\n",
    "    x_11 = conv_block(init_layer, nf*16, (1, 1), pool_size = (2, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x_11 = conv_block(x_11, nf*8, fs_11, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_11 = conv_block(x_11, nf*16, fs_11, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_11 = ZeroPadding2D(padding = (1, 0))(x_11)\n",
    "\n",
    "    x_13 = conv_block(init_layer, nf*16, (1, 1), pool_size = (2, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x_13 = conv_block(x_13, nf*8, fs_13, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_13 = conv_block(x_13, nf*16, fs_13, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_13 = ZeroPadding2D(padding = ((3, 0), 0))(x_13)\n",
    "    \n",
    "    x_15 = conv_block(init_layer, nf*16, (1, 1), pool_size = (2, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x_15 = conv_block(x_15, nf*8, fs_15, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_15 = conv_block(x_15, nf*16, fs_15, pool_size = (2, 1), padding = padding, activation = activation)\n",
    "    x_15 = ZeroPadding2D(padding = ((5, 0), 0))(x_15)\n",
    "\n",
    "    concat_layer = Concatenate()([x_3, x_5, x_7, x_9, x_11, x_13, x_15])\n",
    "    x = conv_block(concat_layer, nf*32, (1, 1), padding = padding, activation = activation, downsample = False)\n",
    "    x = conv_block(x, nf*16, fs_3)\n",
    "    x = conv_block(x, nf*32, fs_3)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(activation)(x)\n",
    "    outputs = Dense(4)(x)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "    optimizer = tensorflow.keras.optimizers.Adam()\n",
    "    \n",
    "    # M과 V는 따로 학습시킬 수 있도록 합니다.\n",
    "    global weight2\n",
    "    if train_target == 1: # only for M\n",
    "        weight2 = np.array([0,0,1,0])\n",
    "    elif train_target == 2: # only for V\n",
    "        weight2 = np.array([0,0,0,1])\n",
    "       \n",
    "    if train_target==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Training >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AseMcHzFL7OJ"
   },
   "outputs": [],
   "source": [
    "def train(model,X,Y, X_val, Y_val, train_target, fold_num, epochs = 200):\n",
    "    # 폴더가 존재하지 않으면 생성합니다.\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "    # target에 따라 다른 모델을 저장합니다.\n",
    "    if train_target == 0:\n",
    "        model_path = MODEL_SAVE_FOLDER_PATH + f'xymodel{fold_num}.hdf5'\n",
    "    elif(train_target == 1):\n",
    "        model_path = MODEL_SAVE_FOLDER_PATH + f'mmodel{fold_num}.hdf5'\n",
    "    else:\n",
    "        model_path = MODEL_SAVE_FOLDER_PATH + f'vmodel{fold_num}.hdf5'\n",
    "\n",
    "    best_save = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    callbacks = [best_save, ReduceLROnPlateau(monitor='val_loss', factor = 0.1, min_lr = 1e-6, patience = 20, verbose = 1)]\n",
    "\n",
    "    history = model.fit(X, Y,\n",
    "                        epochs=epochs, batch_size=256,\n",
    "                        shuffle=True, verbose = 2,\n",
    "                        validation_data = (X_val, Y_val),\n",
    "                        callbacks=[callbacks])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzrqRg4AL9Zk"
   },
   "outputs": [],
   "source": [
    "# 테스트를 위한 지표 정의\n",
    "def E1(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: distance error normalized with 2e+04\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
    "    \n",
    "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
    "\n",
    "\n",
    "def E2(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))\n",
    "\n",
    "def load_best_model(train_target, fold_num):\n",
    "\n",
    "    if train_target == 0:\n",
    "        model = load_model(f'./model/xymodel{fold_num}.hdf5' , custom_objects={'my_loss_E1': my_loss, 'Mish':Mish})\n",
    "    elif(train_target == 1):\n",
    "        model = load_model(f'./model/mmodel{fold_num}.hdf5', custom_objects = {'my_loss_E2':my_loss, 'Mish':Mish})\n",
    "    else:\n",
    "        model = load_model(f'./model/vmodel{fold_num}.hdf5' , custom_objects={'my_loss_E2': my_loss, 'Mish':Mish})\n",
    "\n",
    "    score = model.evaluate(X_data, Y_data, verbose=0)\n",
    "    print('loss:', score)\n",
    "\n",
    "    pred = model.predict(X_data)\n",
    "\n",
    "    # show only one sample\n",
    "    i=0\n",
    "    print('정답(original):', Y_data[i])\n",
    "    print('예측값(original):', pred[i])\n",
    "\n",
    "    print(E1(pred, Y_data))\n",
    "    print(E2(pred, Y_data))\n",
    "#     print(E2M(pred, Y_data))\n",
    "#     print(E2V(pred, Y_data))    \n",
    "    \n",
    "#     if train_target ==0:\n",
    "#         plot_error(4,pred,Y_data)\n",
    "#     elif train_target ==1:\n",
    "#         plot_error(2,pred,Y_data)\n",
    "#     elif train_target ==2:\n",
    "#         plot_error(3,pred,Y_data)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 주로 100 ~ 150 epoch 사이에서 수렴을 보이기 때문에 최대 200 epoch으로 설정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14072886,
     "status": "ok",
     "timestamp": 1594064338039,
     "user": {
      "displayName": "조휘용",
      "photoUrl": "",
      "userId": "03176435450859375087"
     },
     "user_tz": -540
    },
    "id": "8zmpKLAwMJE3",
    "outputId": "016bd19f-85f6-4d67-d325-055e3f99e5e0"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "epochs = 200\n",
    "\n",
    "kfold = KFold(n_splits = 5, random_state = 777)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "    x_train_fold, x_val_fold = X_train[train_idx], X_train[val_idx]y_train_fold, y_val_fold = Y_train[train_idx], Y_train[val_idx]\n",
    "\n",
    "    pred = []\n",
    "    submit = pd.read_csv('./data/sample_submission.csv')\n",
    "    print(f'fold {i} start!!')\n",
    "\n",
    "    for train_target in range(3):\n",
    "        model = get_model(train_target)\n",
    "        train(model, x_train_fold, y_train_fold, x_val_fold, y_val_fold, train_target, fold_num = i, epochs = epochs)\n",
    "        best_model = load_best_model(train_target, fold_num = i)\n",
    "\n",
    "        pred_data_test = best_model.predict(X_data_test)\n",
    "\n",
    "        if train_target == 0: # x,y 학습\n",
    "            submit.iloc[:,1] = pred_data_test[:,0]\n",
    "            submit.iloc[:,2] = pred_data_test[:,1]\n",
    "        elif train_target == 1: # M 학습\n",
    "            submit.iloc[:,3] = pred_data_test[:,2]\n",
    "        elif train_target == 2: # V 학습\n",
    "            submit.iloc[:,4] = pred_data_test[:,3]\n",
    "\n",
    "    preds.append(submit.to_numpy()[:, 1:])\n",
    "    print(f'fold {i} End!!')\n",
    "    \n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Submission >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2k19jZ3xKH_q"
   },
   "outputs": [],
   "source": [
    "preds = np.mean(preds, axis = 0)\n",
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "submit.iloc[:,1] = preds[:, 0]\n",
    "submit.iloc[:,2] = preds[:, 1]\n",
    "submit.iloc[:,3] = preds[:, 2]\n",
    "submit.iloc[:,4] = preds[:, 3] \n",
    "\n",
    "submit.to_csv('./submit_current.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "0.00518.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
